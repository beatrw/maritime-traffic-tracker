
# Arquitetura e Processos

## Storage

O storage está dividido entre Data Lake e Banco de Dados, representando o MongoDB
e o SQLServer, respectivamente.

![MongoDB](images/mongoDB.png)


## Data Lake

O Data Lake segue uma arquitetura de camadas, definida como:

**Camada Landing**: Onde os dados brutos vão ser salvos, aqui foi escolhido salvar
os dados em parquet, para economizar espaço no storage.
**Camada Trusted**: Dados no formato parquet, com todas as tabelas juntas por tema
e com títulos de colunas padronizados, dados sem regras de negócio.

## Banco de Daods

**Camada Business**: Dados com regras de negócio, relacionamento entre tabelas e
definição de tipos de dados específicos.


## Tasks

As tarefas são estruturadas para que todo o fluxo esteja organizado em funções bem definidas e chamadas a partir da função `execute` do fluxo. Todas as funções possuem docstrings explicando seu funcionamento, parâmetros e retornos.

**get_params**
Tarefa responsável por calcular os parametros de ano e passá-los para a landing.

**landing_task**
Tarefa dinâmica para captura de dados, podendo ser executada de forma paralela.
Para essa tarefa, foi utilizado o endpoint para capturar todas as tabelas do ano.

**transform_task**
Tarefa para limpeza e transformação dos dados.
Nesta etapa, os dados são lidos por tema e juntam-se todos os dataframes dos anos,
de acordo com o tema. Colunas são renomeadas para padrão snake_case.

**business_atracacao_task**
Tarefa para injetar regras de negócio nos dados de *Atracação*.
É realizado um relacionamento com a tabela de tempo de atracação, para captura
de informações relativas a períodos de atracação, além de definição de tipos de
dados para colunas específicas de data e float.

**business_carga_task**
Tarefa para injetar regras de negócio nos dados de *Carga*.
São realizados relacionamentos com as tabelas de atracação e carga conteinerizada,
para captura de informações relativas a atracação e detalhes sobre carga, como
mercadoria e peso líquido de carga.

## Plugins

Funções auxiliares são criadas e organizadas na pasta de plugins para garantir escalabilidade no ambiente `airflow`, mantendo essas ferramentas mais genéricas dentro do projeto e não vinculadas a uma DAG específica.


**logging_config**
Função auxiliar para configuração de logs.

**spark_config**
Função auxiliar para configuração de sessões Spark.

**storage_paths**
Funções auxiliares para formatação de pastas e listagem de arquivos.

